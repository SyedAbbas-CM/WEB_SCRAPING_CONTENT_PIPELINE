# ScrapeHive Configuration
# Copy this to .env and fill in your values

# =============================================================================
# COMPLIANCE SETTINGS
# =============================================================================
# Controls scraping behavior:
# - api_only: Only use official APIs (most compliant)
# - api_first: Prefer APIs, fallback to compliant scraping
# - balanced: Mix of API and ethical scraping
# - aggressive: Full anti-detection (use carefully)
COMPLIANCE_MODE=api_first

# =============================================================================
# REDIS CONFIGURATION
# =============================================================================
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_PASSWORD=
REDIS_DB=0

# =============================================================================
# MASTER NODE
# =============================================================================
MASTER_IP=192.168.1.100
MASTER_PORT=5000

# =============================================================================
# API CREDENTIALS
# =============================================================================

# Reddit API (https://www.reddit.com/prefs/apps)
# Rate Limits: 100 requests/minute (authenticated), 10/minute (unauthenticated)
REDDIT_CLIENT_ID=your_reddit_client_id_here
REDDIT_CLIENT_SECRET=your_reddit_client_secret_here
REDDIT_USERNAME=your_reddit_username
REDDIT_PASSWORD=your_reddit_password

# YouTube Data API v3 (https://console.developers.google.com)
# Daily Quota: 10,000 units/day (search = 100 units, video details = 1 unit)
YOUTUBE_API_KEY=your_youtube_api_key_here

# Twitter API v2 (https://developer.twitter.com)
# Rate Limits: 450 search requests per 15 minutes
TWITTER_BEARER_TOKEN=your_twitter_bearer_token_here
TWITTER_API_KEY=your_twitter_api_key_here
TWITTER_API_SECRET=your_twitter_api_secret_here
TWITTER_ACCESS_TOKEN=your_twitter_access_token_here
TWITTER_ACCESS_TOKEN_SECRET=your_twitter_access_token_secret_here

# Instagram Graph API (https://developers.facebook.com)
# Rate Limits: 200 calls per user per hour
INSTAGRAM_ACCESS_TOKEN=your_instagram_access_token_here
INSTAGRAM_APP_ID=your_instagram_app_id_here
INSTAGRAM_APP_SECRET=your_instagram_app_secret_here

# Facebook Graph API (https://developers.facebook.com)
# Rate Limits: 4800 calls per 24 hours (varies by impressions)
FACEBOOK_ACCESS_TOKEN=your_facebook_access_token_here
FACEBOOK_APP_ID=your_facebook_app_id_here
FACEBOOK_APP_SECRET=your_facebook_app_secret_here

# TikTok Research API (https://developers.tiktok.com)
# Rate Limits: 1000 requests/day (Research), 10000/day (Commercial)
TIKTOK_CLIENT_KEY=your_tiktok_client_key_here
TIKTOK_CLIENT_SECRET=your_tiktok_client_secret_here

# =============================================================================
# PROXY CONFIGURATION (Optional but recommended)
# =============================================================================
# Proxy providers: brightdata, oxylabs, smartproxy, etc.
PROXY_PROVIDER=
PROXY_USERNAME=
PROXY_PASSWORD=
PROXY_ENDPOINT=

# Residential proxy endpoints (format: protocol://user:pass@endpoint:port)
RESIDENTIAL_PROXY_HTTP=
RESIDENTIAL_PROXY_HTTPS=

# Datacenter proxy endpoints
DATACENTER_PROXY_HTTP=
DATACENTER_PROXY_HTTPS=

# Mobile proxy endpoints
MOBILE_PROXY_HTTP=
MOBILE_PROXY_HTTPS=

# =============================================================================
# AI MODEL CONFIGURATION
# =============================================================================
# Path to store AI models
MODEL_PATH=/path/to/models
WHISPER_MODEL=base
DEVICE=cuda  # or cpu

# OpenAI API (for content analysis if no local models)
OPENAI_API_KEY=your_openai_api_key_here

# Hugging Face (for model downloads)
HUGGINGFACE_TOKEN=your_huggingface_token_here

# =============================================================================
# RATE LIMITING CONFIGURATION
# =============================================================================
# Global rate limits (requests per hour)
MAX_REQUESTS_PER_HOUR=1000

# Platform-specific limits (override API defaults if needed)
REDDIT_RATE_LIMIT=90    # Stay under 100/min limit
TWITTER_RATE_LIMIT=400  # Stay under 450/15min limit
YOUTUBE_RATE_LIMIT=9000 # Stay under 10k daily quota
INSTAGRAM_RATE_LIMIT=180 # Stay under 200/hour limit
FACEBOOK_RATE_LIMIT=4500 # Stay under daily limit
TIKTOK_RATE_LIMIT=900    # Stay under daily limit

# =============================================================================
# DATABASE CONFIGURATION
# =============================================================================
DATABASE_PATH=~/.scrapehive/master.db
DATABASE_URL=sqlite:///~/.scrapehive/master.db

# PostgreSQL (optional, for production)
# DATABASE_URL=postgresql://user:password@localhost:5432/scrapehive

# =============================================================================
# LOGGING CONFIGURATION
# =============================================================================
LOG_LEVEL=INFO
LOG_PATH=~/.scrapehive/logs

# =============================================================================
# SECURITY CONFIGURATION
# =============================================================================
SECRET_KEY=your-secret-key-here-make-it-long-and-random
API_KEY=your-api-key-for-external-access

# JWT settings
JWT_SECRET_KEY=your-jwt-secret-key-here
JWT_EXPIRATION_HOURS=24

# =============================================================================
# WEB DASHBOARD CONFIGURATION
# =============================================================================
DASHBOARD_HOST=0.0.0.0
DASHBOARD_PORT=5000
DASHBOARD_DEBUG=False

# =============================================================================
# SCRAPING BEHAVIOR CONFIGURATION
# =============================================================================
# Delays between requests (seconds)
MIN_SCRAPE_DELAY=2
MAX_SCRAPE_DELAY=5

# Human-like behavior variance
HUMAN_VARIANCE=0.3

# Browser settings for fallback scraping
BROWSER_HEADLESS=True
BROWSER_TIMEOUT=30

# User agent rotation
ROTATE_USER_AGENTS=True
CUSTOM_USER_AGENT="ScrapeHive/1.0 (+https://scrapehive.com/bot)"

# =============================================================================
# CONTENT STORAGE CONFIGURATION
# =============================================================================
# Where to store scraped content
SCRAPED_DATA_PATH=data/scraped
PROCESSED_DATA_PATH=data/processed

# Content retention (days)
SCRAPED_DATA_RETENTION=30
PROCESSED_DATA_RETENTION=90

# File size limits (MB)
MAX_FILE_SIZE=10
MAX_IMAGE_SIZE=5
MAX_VIDEO_SIZE=100

# =============================================================================
# MONITORING & ALERTS
# =============================================================================
# Metrics collection interval (seconds)
METRICS_INTERVAL=60

# Alert thresholds
CPU_ALERT_THRESHOLD=90
MEMORY_ALERT_THRESHOLD=85
ERROR_RATE_ALERT_THRESHOLD=0.1
QUEUE_SIZE_ALERT_THRESHOLD=1000

# Email alerts (optional)
SMTP_HOST=
SMTP_PORT=587
SMTP_USERNAME=
SMTP_PASSWORD=
ALERT_EMAIL_TO=admin@example.com
ALERT_EMAIL_FROM=scrapehive@example.com

# Slack alerts (optional)
SLACK_WEBHOOK_URL=
SLACK_CHANNEL=#alerts

# =============================================================================
# DEVELOPMENT/DEBUGGING
# =============================================================================
DEBUG=False
TESTING=False

# Enable/disable specific features for debugging
ENABLE_RATE_LIMITING=True
ENABLE_PROXY_ROTATION=True
ENABLE_ANTI_DETECTION=True
ENABLE_CACHING=True

# Mock API responses for testing
MOCK_REDDIT_API=False
MOCK_YOUTUBE_API=False
MOCK_TWITTER_API=False

# =============================================================================
# LEGAL & ETHICAL COMPLIANCE
# =============================================================================
# Respect robots.txt files
RESPECT_ROBOTS_TXT=True

# Identify as bot in user agents
IDENTIFY_AS_BOT=True

# Maximum requests per website per hour (ethical limit)
MAX_REQUESTS_PER_SITE_PER_HOUR=100

# Avoid scraping during peak hours (format: HH:MM-HH:MM)
AVOID_PEAK_HOURS=False
PEAK_HOURS_START=12:00
PEAK_HOURS_END=14:00

# Geographic restrictions (comma-separated country codes)
BLOCKED_COUNTRIES=
ALLOWED_COUNTRIES=

# =============================================================================
# PERFORMANCE OPTIMIZATION
# =============================================================================
# Number of worker threads
MAX_WORKERS=4

# Connection pooling
CONNECTION_POOL_SIZE=10
CONNECTION_POOL_MAX_SIZE=20

# Caching
ENABLE_RESPONSE_CACHE=True
CACHE_EXPIRATION=300  # 5 minutes

# Compression
ENABLE_COMPRESSION=True

# =============================================================================
# BACKUP & RECOVERY
# =============================================================================
# Automatic backup settings
ENABLE_AUTO_BACKUP=True
BACKUP_INTERVAL_HOURS=24
BACKUP_RETENTION_DAYS=7
BACKUP_PATH=backups/

# Recovery settings
ENABLE_AUTO_RECOVERY=True
RECOVERY_CHECK_INTERVAL=60  # seconds